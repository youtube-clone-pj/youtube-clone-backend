# 라이브 스트리밍 채팅 전송 최적화 - 쓰기 시 DB 조회(SELECT) 3회 → 0회로 100% 제거

## 목차
1. [배경 및 문제 정의](#1-배경-및-문제-정의)
2. [해결 전략: 쓰기 시점의 DB 조회 제거](#2-해결-전략-쓰기-시점의-db-조회-제거)
3. [구현 상세](#3-구현-상세)
4. [최적화 결과](#4-최적화-결과)

---

## 1. 배경 및 문제 정의

### 1.1 프로젝트 상황
라이브 스트리밍의 핵심 기능인 '실시간 채팅'은 방송 시작 직후나 종료 직전 등 특정 시점에 **트래픽이 급증**하는 특성을 가집니다.

### 1.2 기존 성능 문제
이전 단계에서 채팅 **조회(Read)** 성능은 인덱스 최적화를 통해 **0.357ms**까지 개선했습니다. 그러나 채팅 **전송(Write)** 로직을 분석한 결과, 메시지 1건을 전송할 때마다 불필요한 **DB 조회가 2회 발생**하고 있음을 확인했습니다.

#### [기존 로직의 DB 부하]
1. **User 조회**: 채팅 작성자의 닉네임, 프로필 이미지를 역정규화하기 위해 매번 User 엔티티 조회
2. **Status 조회**: 방송이 진행 중인지 확인하기 위해 LiveStreaming 엔티티 조회 (Status 검증)

불필요한 DB 조회가 많을수록 **DB 커넥션 점유 시간이 길어지며**, 이는 동시 요청이 몰리는 상황에서 HikariCP 커넥션 획득 대기로 이어져 전체적인 응답 지연의 원인이 됩니다.

---

## 2. 해결 전략: 쓰기 시점의 DB 조회 제거

채팅 전송은 빈번하게 발생하는 '쓰기' 작업이므로, **API 응답 시간을 단축하여 DB 커넥션 점유 시간을 최소화하는 것**을 목표로 설정했습니다. 이를 위해 **추가적인 DB 조회를 0회**로 최적화했습니다.

### 2.1 전략 1: 세션 정보 재사용 (User 정보)
- **문제**: 이미 인증 필터와 컨트롤러 단계에서 세션을 통해 사용자 정보를 알고 있음에도, 서비스 계층에서 DB를 다시 조회함.
- **해결**: 컨트롤러에서 세션의 사용자 정보(`username`, `profileImageUrl`)를 추출하여 서비스 메서드의 인자로 직접 전달.

### 2.2 전략 2: 로컬 캐시 활용 (Status 정보)
- **문제**: 채팅 전송 시 해당 방송이 '실제 진행 중'인지 확인하는 검증 로직이 필수적입니다. 라이브 상태(`LIVE`, `ENDED`)는 방송 시작/종료 시에만 변경되는 빈도가 극히 낮은 데이터임에도, 모든 채팅 전송 시마다 상태 확인을 위해 DB를 조회하는 것은 비효율적입니다.
- **해결**:
    - **캐시 라이브러리**: Redis 대비 **수백 배 이상의 조회 성능**을 가진 **Caffeine Cache**(로컬 캐시)를 적용했습니다.
    - **복합 캐시 전략 적용**:
        - **Look-Aside (읽기 전략)**: 채팅 전송 전, 로컬 캐시에서 방송 상태를 우선 확인합니다. 캐시에 데이터가 없을 때만 DB를 조회하고 결과를 캐시에 저장합니다.
        - **Write-Through (쓰기 전략)**: 방송 시작/종료 시점에 이벤트를 발행하여 캐시를 즉시 업데이트합니다. 이를 통해 DB와 캐시의 데이터 일관성을 유지하고, 갑작스러운 DB 조회가 몰리는 현상(Cache Stampede)을 방지합니다.

#### 캐시 만료 및 관리 전략 (Dynamic TTL & Cache Stampede)
캐시 만료 시 다수의 요청이 동시에 DB를 조회하고 중복으로 캐시 갱신을 시도하는 **캐시 스탬피드(Cache Stampede)** 현상을 방지하기 위해, 라이브 스트리밍의 라이프사이클에 맞춰 TTL을 동적으로 관리합니다.

- **방송 시작 시 (TTL 12시간)**: 방송 중 캐시가 만료되어 갑자기 DB 조회가 발생하는 일을 원천 차단하기 위해 충분히 긴 시간(12시간)을 설정합니다.
- **방송 종료 시 (TTL 30분)**: 방송 종료 직후에도 캐시를 즉시 삭제하지 않고 30분간 유지합니다.
    - **도메인 특성 반영**: 라이브 스트리밍은 종료 직전에 시청자들이 작별 인사(예: "수고하셨습니다", "ㅂㅂ")를 쏟아내기 때문에, 오히려 방송 중보다 더 높은 트래픽 스파이크가 발생할 수 있습니다. 
    - **안정성 확보**: 종료 후 30분간 캐시를 유지하여, 종료 시점의 폭발적인 채팅 요청이 DB 부하로 이어지지 않도록 방어하고 이후 메모리를 효율적으로 회수합니다.

### 2.3 전략 3: JPA 프록시 활용 (Entity 참조)
- **문제**: 채팅 엔티티 저장 시 `LiveStreaming` 연관 관계 설정을 위해 `findById`로 조회하면 SELECT 쿼리 발생.
- **해결**: `getReferenceById`를 사용하여 프록시 객체만 획득, SELECT 쿼리 없이 FK 설정.

---

## 3. 구현 상세

### 3.1 Controller: 세션 정보 전달
컨트롤러에서 세션에 저장된 사용자 정보를 추출하여 서비스로 넘깁니다. 불필요한 User 조회를 원천 차단했습니다.

```java
// LiveStreamingV2Controller.java
@PostMapping("/{liveStreamingId}/chats")
public ResponseEntity<LiveStreamingChatInfo> sendChat(
        @PathVariable final Long liveStreamingId,
        @RequestBody final ChatMessageRequest request,
        final HttpSession session
) {
    // 세션에서 사용자 정보 직접 추출 (DB 조회 제거)
    final Long userId = (Long) session.getAttribute(SESSION_USER_ID);
    final String username = (String) session.getAttribute(SESSION_USERNAME);
    final String profileImageUrl = (String) session.getAttribute(SESSION_PROFILE_IMAGE_URL);
    
    // 서비스 호출 시 정보 전달
    final LiveStreamingChatInfo chatInfo = liveStreamingChatService.sendMessage(
            liveStreamingId, userId, username, profileImageUrl, // 전달
            request.getMessage(), request.getChatMessageType(), Instant.now()
    );
    // ...
}
```

### 3.2 Service: 캐시 및 프록시 활용
서비스 계층에서는 전달받은 정보를 사용하고, 라이브 상태 확인 및 엔티티 참조 시 DB 조회를 방지합니다.

```java
// LiveStreamingChatService.java
@Transactional
public LiveStreamingChatInfo sendMessage(
        final Long liveStreamingId, final Long userId, final String username,
        final String profileImageUrl, final String message, 
        final ChatMessageType messageType, final Instant now
) {
    // 1. 상태 검증: 로컬 캐시 조회 (DB 조회 X)
    final LiveStreamingStatus status = liveStreamingReader.readCachedStatusBy(liveStreamingId);
    
    // 2. 연관 관계 설정: 프록시 객체 획득 (DB 조회 X)
    final LiveStreaming liveStreaming = liveStreamingReader.getReferenceBy(liveStreamingId);

    // 3. 채팅 저장 (INSERT 1회만 발생)
    liveStreamingChatWriter.write(
            liveStreaming, status, userId, username, profileImageUrl, message, messageType
    );

    return LiveStreamingChatInfo.of(userId, username, profileImageUrl, message, messageType, now);
}
```

### 3.3 Cache Strategy: Look-Aside + Write-Through
`@Cacheable`로 조회 성능을 높이고, 스프링 AOP 특성을 고려하여 `@TransactionalEventListener`를 통해 트랜잭션이 성공적으로 커밋된 직후 캐시를 동기화하여 데이터 일관성을 보장했습니다.

```java
// LiveStreamingReader.java
@Cacheable(value = "liveStreamingStatus", key = "#liveStreamingId", sync = true)
public LiveStreamingStatus readCachedStatusBy(final Long liveStreamingId) {
    return liveStreamingRepository.findStatusById(liveStreamingId)
            .orElseThrow(() -> new BaseException(LiveStreamingErrorCode.LIVE_STREAMING_NOT_FOUND));
}

// LiveStreamingStatusCacheEventListener.java
@TransactionalEventListener(phase = TransactionPhase.AFTER_COMMIT)
public void handleStatusChanged(final LiveStreamingStatusChangedEvent event) {
    // 방송 시작/종료 직후 캐시 즉시 업데이트 -> Cache Stampede 방지
    cacheUpdater.updateCache(event.liveStreamingId(), event.status());
}
```

---

## 4. 최적화 결과

### 4.1 개선 전후 비교 (채팅 1건 전송 시)

| 항목 | 개선 전 | 개선 후 | 비고 |
| :--- | :---: | :---: | :--- |
| **User 조회** | 1회 (SELECT) | **0회** | Session 정보 활용 |
| **Status 조회** | 1회 (SELECT) | **0회** | Caffeine Local Cache |
| **Entity 참조** | 1회 (SELECT) | **0회** | `getReferenceById` (Proxy) |
| **Chat 저장** | 1회 (INSERT) | 1회 (INSERT) | 필수 작업 |
| **Total Query** | **3회** | **1회** | **66% 감소 (조회 쿼리 100% 제거)** |

### 4.2 비즈니스 임팩트
1. **DB 커넥션 효율화**: 불필요한 DB 조회를 제거하여 **커넥션 점유 시간을 단축**시켰으며, 이는 한정된 커넥션 풀 자원을 더 많은 요청이 효율적으로 사용할 수 있게 합니다.
2. **안정성 확보**: 방송 시작 직후 다수의 시청자가 동시에 채팅을 시도하는 상황에서도, DB 조회 없이 캐시와 메모리 내에서 유효성 검증을 처리하여 시스템이 다운되지 않도록 보호했습니다.
3. **응답 속도 향상**: 네트워크 I/O(DB 통신)를 최소화함으로써 채팅 전송 Latency를 낮추고 전반적인 사용자 경험을 개선했습니다.
